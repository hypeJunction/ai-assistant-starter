[
  {
    "slide": 1,
    "text": "AI Assistant Starter. Before I show you what this framework does, let me share why I built it. Because the hype around AI coding tools doesn't match reality."
  },
  {
    "slide": 2,
    "text": "The METR study measured developers using AI tools against a control group. The results were surprising. Those who felt twenty percent faster were actually nineteen percent slower when measured objectively. The productivity gains aren't real."
  },
  {
    "slide": 3,
    "text": "Where does the time actually go? More than sixty percent fixing what the AI got wrong. Code reviews take nearly half again as long. Pull requests balloon in size. You're not saving time. You're trading typing for debugging."
  },
  {
    "slide": 4,
    "text": "Here's the paradox. Most developers use these tools. But only a third trust what comes out. Forty two percent of AI-generated code contains hallucinated functions or APIs. That's not a productivity tool. That's a liability."
  },
  {
    "slide": 5,
    "text": "There's a fundamental mismatch here. We invented programming languages to escape ambiguity. Natural language is inherently fuzzy. Code must be precise. Make it faster, you say. Which part? By how much? The model will guess. We're using a tool built on probability to produce artefacts that require certainty."
  },
  {
    "slide": 6,
    "text": "So I built AI Assistant Starter. It's a collection of agentic primitives, reusable building blocks that constrain LLM behaviour. Workflows with approval gates. Role constraints for unambiguous scope. Persistent context. And course correction at every phase."
  },
  {
    "slide": 7,
    "text": "The framework is built on three principles. First, humans stay in control through explicit approval gates. Second, unambiguous scope through defined phases and roles. Third, context persists across sessions. Memory survives."
  },
  {
    "slide": 8,
    "text": "The architecture has two layers. The base layer contains shared primitives. Workflows, chatmodes, domains, guidelines. It's versioned and can be updated independently. The project layer contains your specific context. Architecture decisions, domain knowledge. This stays gitignored and travels with your project."
  },
  {
    "slide": 9,
    "text": "At the heart of the framework is atomic composition. Think of it like atomic design principles. A task is the smallest unit of work. Each task has disambiguated intent, project-aware context, and constrained tool access. The word find is vague. The command grep dash r is precise. Workflows are composed of these atomic tasks. Slash implement combines explore, plan, and code into a single orchestrated flow."
  },
  {
    "slide": 10,
    "text": "Every task follows a structured workflow. Explore first in read-only mode. Then plan without writing code. Then hit an approval gate where you explicitly approve or reject. Only after your yes does the model write code. Finally, verified commits. Each phase has clear boundaries. No skipping ahead."
  },
  {
    "slide": 11,
    "text": "Role constraints define exactly what the LLM can do. Each task gets exactly one chatmode. An Explorer can read but not write. A Planner can design but not code. A Developer is limited to specific files. No role confusion. No scope creep. These constraints prevent the creative tangents that derail sessions."
  },
  {
    "slide": 12,
    "text": "Each task gets scoped file access. When you ask the AI to add user validation, it can only touch the files you specify. User feature files, validation utilities. Everything else is blocked. Core modules, config files, off limits. The AI can't refactor what it can't touch. Limited blast radius means limited damage."
  },
  {
    "slide": 13,
    "text": "The approval gate is explicit and unambiguous. After presenting a plan, the LLM asks and waits. Valid approvals are clear affirmatives. Yes. Approved. Proceed. Anything ambiguous like sounds good or maybe doesn't count. Neither does silence."
  },
  {
    "slide": 14,
    "text": "Context lives in a markdown file that captures your project's DNA. When you document your API routes, your authentication approach, your database setup, that context carries forward. New sessions start with understanding, not explanations."
  },
  {
    "slide": 15,
    "text": "This solves a bigger problem too. Domain knowledge usually gets scattered across the team. Alice knows authentication. Bob knows billing. Carol left and took her knowledge with her. Need something? Ask Dave. With the framework, the full story lives in the repository. Product evolution is captured and versioned. No more tribal knowledge that walks out the door."
  },
  {
    "slide": 16,
    "text": "Documentation always drifts from reality. You write it once, then the code changes, and the docs go stale. The framework includes a sync command. AI scans the codebase, compares it against your context file, flags what's outdated, and proposes updates for your approval. Documentation that actually stays current."
  },
  {
    "slide": 17,
    "text": "Here's something powerful. The AI generates and maintains its own instructions. When it produces an undesired output, you tell it what went wrong. It updates the instructions. That mistake won't happen again. Think of it as regression testing for AI behaviour. The instructions evolve and improve over time, just like a test suite catches bugs before they ship."
  },
  {
    "slide": 18,
    "text": "When writing instructions, use positive prompting. Tell the AI what to do, not what to avoid. Negatives are ambiguous. Don't write verbose code could mean anything. But write concise functions under twenty lines is concrete and measurable. Specific affirmatives give the model a clear target to hit."
  },
  {
    "slide": 19,
    "text": "It's all about templating. Point the AI to specific files in your codebase that represent best practices. Don't say write a React component and get generic patterns from training data. Say follow this file and get your conventions, your patterns. Don't let the AI invent. Let it replicate what already works."
  },
  {
    "slide": 20,
    "text": "Here's an uncomfortable truth. We all know what a proper software development lifecycle looks like. Exploration before coding. Planning before building. Testing before shipping. Documentation for maintenance. But under pressure, we cut corners. We skip steps. The code is self-documenting, we say. With AI in the loop, every step actually happens. The framework enforces the discipline we know we need but rarely maintain."
  },
  {
    "slide": 21,
    "text": "Test plans in natural language serve double duty. The framework uses Gherkin-style scenarios as comments at the top of test files. Given, When, Then. It's readable documentation right in the code. Given valid credentials. When user submits login. Then token returned. But it's also a spec that AI can turn into actual test implementations below. Write the scenario in plain English, then let AI generate the test code. For hard-to-test edge cases, the plan evolves alongside your understanding. Documentation that becomes implementation."
  },
  {
    "slide": 22,
    "text": "Commands serve as entry points. Instead of freeform conversation like help me with this, you use structured commands. Slash implement. Slash review. Slash commit. Each command triggers a predefined workflow with all the necessary steps baked in. No ambiguity about intent. No missed steps. The workflow handles the structure so you can focus on the work."
  },
  {
    "slide": 23,
    "text": "This leads to a human-first workflow. You write the mission-critical code. The parts that need your expertise, your judgment, your understanding of the domain. Then AI follows up. It generates tests for your code. It writes documentation. It replicates your pattern to similar files. Focus your expertise where it matters. Delegate the mechanical follow-up work."
  },
  {
    "slide": 24,
    "text": "Some tasks are genuine good fits for LLMs. Boilerplate code is well-defined and verifiable. Test scaffolding follows known patterns. Documentation drafts can be reviewed and refined. Regex patterns are easy to test. What these share: they're mechanical, not creative."
  },
  {
    "slide": 25,
    "text": "But know when to keep tasks human. Novel architecture needs deep thinking. Security-critical code is too high stakes. Complex debugging requires intuition. Core business logic must be fully understood by you. The model assists. It doesn't replace judgment."
  },
  {
    "slide": 26,
    "text": "Getting started takes five minutes. Add the repository as a git submodule. Then run slash init to set up your project layer. The framework handles the rest. You're up and running."
  },
  {
    "slide": 27,
    "text": "Questions? The repository is on GitHub. Give it a try. It won't solve everything, but it makes working with LLMs less frustrating. Pragmatism over hype."
  }
]
