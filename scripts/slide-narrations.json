[
  {
    "slide": 1,
    "text": "I want to have an honest conversation about large language models in software engineering. Not the hype. Not the marketing. Just what I've actually learned using these tools over the past year."
  },
  {
    "slide": 2,
    "text": "Before we go further, let's agree on terminology. What gets called AI today is actually a very specific thing. Large language models are statistical text predictors, a small subset of machine learning. The term AI has been hijacked. Decades of research in robotics, computer vision, and reinforcement learning, all reduced to a buzzword."
  },
  {
    "slide": 3,
    "text": "I spent the past year genuinely trying to make LLMs work for me. And there were moments of real awe. Watching code materialise in seconds from a simple description. It felt like we'd crossed some threshold into the future."
  },
  {
    "slide": 4,
    "text": "But here's what nobody talks about. Everyone online was building entire applications with a few keystrokes. Tech companies claimed they'd replaced half their workforce. Meanwhile, I was struggling with the basics. Surely I was doing something wrong. Surely my prompts were the problem."
  },
  {
    "slide": 5,
    "text": "When I started digging into actual research, I found something troubling. The METR study measured developers using AI tools against a control group. The results were surprising. Those who felt faster were actually slower."
  },
  {
    "slide": 6,
    "text": "This is the number that stuck with me. Nineteen percent slower when measured objectively. Not faster. Slower. Despite everyone feeling like they were flying through tasks."
  },
  {
    "slide": 7,
    "text": "The hidden costs paint an even clearer picture. More than half your time goes to fixing what the AI got wrong. Pull requests balloon in size. Reviews take nearly twice as long. You're not saving time. You're trading typing for debugging."
  },
  {
    "slide": 8,
    "text": "Here's the paradox that keeps me up at night. Most developers are using these tools. But they don't trust what comes out. That's not a productivity tool. That's a liability."
  },
  {
    "slide": 9,
    "text": "Look at this code snippet. Looks reasonable, right? Except that function doesn't exist. Never did. The model invented it with complete confidence. Up to forty two percent of AI-generated code contains these hallucinations. Phantom functions and nonexistent APIs."
  },
  {
    "slide": 10,
    "text": "The security implications are alarming. The model learned from the entire internet, including all the bad code, the vulnerable patterns, the exposed secrets. And it reproduces them confidently."
  },
  {
    "slide": 11,
    "text": "There's a fundamental mismatch here. We invented programming languages to escape ambiguity. Natural language is inherently fuzzy. Code must be precise. We're using a tool built on probability to produce artefacts that require certainty. Make it faster, you say. Which part? By how much? The model will guess."
  },
  {
    "slide": 12,
    "text": "Too many of us are blaming ourselves for bad prompts when the tool itself is fundamentally limited. We assume everyone else has figured it out because they're not complaining publicly. But they're having the same problems. They're just not saying it."
  },
  {
    "slide": 13,
    "text": "I don't want to discount LLMs entirely. The potential is real. I'm happy to delegate mundane tasks to them. But they work better with explicit, structured instructions than with freeform conversation. The tool needs guardrails. And if nobody else is providing them, we need to build them ourselves."
  },
  {
    "slide": 14,
    "text": "That's why I built AI Assistant Starter. It's not a silver bullet. It's a collection of agentic primitives, reusable building blocks that constrain LLM behaviour. Think of it as damage control, not a cure."
  },
  {
    "slide": 15,
    "text": "The framework is built on three principles. First, humans stay in control through explicit approval gates. Second, structure reduces chaos through defined phases and roles. Third, context persists, so you stop re-explaining the same things every session."
  },
  {
    "slide": 16,
    "text": "The architecture has two layers. The base layer contains shared primitives, workflows, roles, guidelines. It's versioned and can be updated independently. The project layer contains your specific context, your architecture decisions, your patterns. This stays with your project."
  },
  {
    "slide": 17,
    "text": "Every task follows a structured workflow. Explore first, read-only. Then plan, no code allowed yet. Then hit an approval gate where you explicitly approve or reject. Only after your yes does the model write code. Each phase has clear boundaries. No skipping ahead."
  },
  {
    "slide": 18,
    "text": "Chatmodes constrain what the LLM can do in each role. An Explorer can browse the codebase but cannot make changes. A Planner can design but cannot implement. A Developer is limited to specific files. These constraints prevent the creative tangents that derail sessions."
  },
  {
    "slide": 19,
    "text": "The approval gate is explicit and unambiguous. After presenting a plan, the LLM asks for approval and waits. Valid approvals are clear affirmatives. Anything else, including silence, means wait. Sounds good doesn't count."
  },
  {
    "slide": 20,
    "text": "Context lives in a markdown file that captures your project's DNA. Architecture decisions, key patterns, technology choices. When you document that your API routes live under slash api slash v one, that context carries forward. You stop re-explaining every session."
  },
  {
    "slide": 21,
    "text": "Let me be clear about what this framework won't fix. LLMs are still probabilistic at their core. You still need to review everything they produce. And no framework can replace the experience and intuition that takes years to develop."
  },
  {
    "slide": 22,
    "text": "But it does make working with LLMs more manageable. Structured output reduces the verification burden. Constrained roles prevent scope creep. Persistent context means less time spent on setup and more time on actual work."
  },
  {
    "slide": 23,
    "text": "Some tasks are genuinely good fits for LLMs. Boilerplate code, test scaffolding, documentation drafts, regex patterns. What these share in common: they're well-defined, easily verified, and don't require deep understanding of your specific system."
  },
  {
    "slide": 24,
    "text": "But know when to keep tasks human. Novel architecture decisions need deep thinking. Security-critical code is too high stakes for probabilistic output. Complex debugging requires intuition built from experience. Core business logic must be fully understood by you, not delegated."
  },
  {
    "slide": 25,
    "text": "The craft of software engineering remains fundamentally human. That gut feeling when something is off. The deep context of your domain. The accountability for what ships. A model can assist, but it cannot develop these. Your intuition comes from years of mistakes. No shortcut exists."
  },
  {
    "slide": 26,
    "text": "Getting started takes five minutes. Add the repository as a git submodule, then run the init command to set up your project layer. The framework handles the rest."
  },
  {
    "slide": 27,
    "text": "Here's what I want you to take away. Be skeptical of the hype. LLMs need guardrails, not blind trust. Human oversight is non-negotiable. Some tasks still need human craft. And the framework can help make LLMs useful enough to keep around, on your terms."
  },
  {
    "slide": 28,
    "text": "The repository is available on GitHub. Give it a try. It won't solve everything, but it might make working with LLMs less frustrating. Pragmatism over hype. Honesty over marketing. Thanks for listening."
  }
]
